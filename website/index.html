<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>SECURE</title>
    <style>
        h1 {
            font-family: 'Public Sans', sans-serif; 
            color: #098487;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #efeee7;
        }
        .paper-container {
            background-color: white; 
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.5);
        }
        .title-section {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 20px;
            margin-bottom: 30px;
        }
        .title {
            text-align: center;
            font-size: 2em;
            margin: 0;
            flex: 1;
        }
        .authors {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
        }
        .links-section {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
        }
        .external-link {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 8px 16px;
            background-color: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 4px;
            color: #24292e;
            text-decoration: none;
            transition: all 0.2s ease;
        }
        .external-link:hover {
            background-color: #f1f1f1;
            transform: translateY(-1px);
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .abstract {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 4px;
        }
        .section {
            margin-bottom: 30px;
        }
        .section-title {
            color: #098487;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-family: 'MinecraftFont', sans-serif;
        }
        .bibtex-link {
            text-align: center;
            margin: 20px 0;
        }
        .bibtex-link a {
            color: #098487;
            text-decoration: none;
        }
        .bibtex-link a:hover {
            text-decoration: underline;
        }
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .bibtex-section {
            margin-top: 40px;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 4px;
        }
        .bibtex-code {
            background-color: #fff;
            padding: 20px;
            border-radius: 4px;
            border: 1px solid #ddd;
            font-family: 'Courier New', Courier, monospace;
            white-space: pre-wrap;
            overflow-x: auto;
        }
        .copy-button {
            background-color: #098487;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            margin-bottom: 10px;
            font-family: 'Public Sans', sans-serif;
        }
        .copy-button:hover {
            background-color: #098487;
        }
        .icon {
            width: 20px;
            height: 20px;
        }
        .animated-gradient-text {
            background: linear-gradient(270deg, #098487, #00c6cd, #098487);
            background-size: 200% 200%;
            background-clip: text;
            -webkit-background-clip: text;
            color: transparent;
            -webkit-text-fill-color: transparent;
            animation: gradientMove 3s linear infinite;
        }
        @keyframes gradientMove {
            0% {
                background-position: 0% 50%;
            }
            100% {
                background-position: 100% 50%;
            }
        }
        a {
            color: #343464 !important;
            font-style: normal !important;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .semantic-analysis-flex {
            display: flex;
            align-items: flex-start;
            gap: 30px;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .semantic-bullets {
            flex: 1;
            margin: 0;
            padding-left: 20px;
            font-size: 1.05em;
        }
        .semantic-image {
            width: 30%;
            max-width: none;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.08);
        }
        .title-flex {
            display: flex;
            align-items: center;
            gap: 30px;
            margin-bottom: 30px;
        }
    </style>
</head>
<body>
    <div class="paper-container">
         <img src="assets/overview.png" alt="SECURE Overview" style="width: 100%; height: auto;">


        <div class="title-section title-flex">
            <h1 class="title animated-gradient-text"> SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning</h2>
        </div>
        <p style="color: #098487; font-style: italic; text-align: center;">Interactive task learning framework to cope with unforeseen possibilities by exploiting <br> the formal semantic analysis of embodied conversation</p>

        <div class="authors" style="font-size: large;">
            <br>
            <a href="https://rimvydasrub.github.io/">Rimvydas Rubavicius</a>, <a href="https://peterdavidfagan.com/">Peter David Fagan</a>, <a href="https://homepages.inf.ed.ac.uk/alex/">Alex Lascarides</a>, <a href="https://www.research.ed.ac.uk/en/persons/ram-ramamoorthy">Subramanian Ramamoorthy</a><br>
            <p>Centre for AI in Assistive Autonomy</p>
            <p>University of Edinburgh</p>
        </div>
        

        <div class="links-section">
            <a href="https://arxiv.org/abs/2409.17755" class="external-link">
                <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                    <polyline points="14 2 14 8 20 8"></polyline>
                    <line x1="16" y1="13" x2="8" y2="13"></line>
                    <line x1="16" y1="17" x2="8" y2="17"></line>
                    <polyline points="10 9 9 9 8 9"></polyline>
                </svg>
                Paper
            </a>
            <a href="https://github.com/assistive-autonomy/secure" class="external-link">
                <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
                </svg>
                Code
            </a>
        </div>

        <div class="abstract">
            <strong>Abstract:</strong> This paper addresses a challenging interactive task learning scenario we call rearrangement under unawareness: 
            an agent must manipulate a rigid-body environment without knowing a key concept necessary for solving the task and must learn about it during deployment. 
            For example, the user may ask to "put the two granny smith apples inside the basket", but the agent cannot correctly identify which objects in the 
            environment are "granny smith" as the agent has not been exposed to such a concept before. We introduce SECURE, an interactive task learning policy 
            designed to tackle such scenarios. The unique feature of SECURE is its ability to enable agents to engage in semantic analysis when processing embodied 
            conversations and making decisions. Through embodied conversation, a SECURE agent adjusts its deficient domain model by engaging in dialogue to identify 
            and learn about previously unforeseen possibilities. The SECURE agent learns from the user's embodied corrective feedback when mistakes are made and 
            strategically engages in dialogue to uncover useful information about novel concepts relevant to the task. These capabilities enable the SECURE 
            agent to generalize to new tasks with the acquired knowledge. We demonstrate in the simulated Blocksworld and the real-world apple manipulation 
            environments that the SECURE agent, which solves such rearrangements under unawareness, is more data-efficient than agents that do not engage in 
            embodied conversation or semantic analysis.
        </div>

        <div class="section">
            <h2 class="section-title">Introduction</h2>
            <div class="image-container">
                <img src="assets/grounding_dino_predictions.png" alt="Grounding DINO Predictions">
                <br>
                <em>Figure 1: Comparison between <a href="https://github.com/IDEA-Research/GroundingDINO">grounding DINO</a> predictions and ground-truth domain model.</em>
            </div>
            <ul>    
                <li>In real-world scenarios, the robot has to solve <strong>tasks under unawareness</strong> due to uncertainty and false beliefs about the structure and parameters of the domain model (see Figure 1)</li>
                <li> <strong>Embodied conversation</strong> allows one to cope with unawareness by enabling <strong>interactive symbol grounding</strong></li>
                <li>We propose an <strong>interactive task learning</strong> framework that processes embodied conversation using semantic analysis to  make robot <strong>semantics-aware</strong></li>
            </ul>
        </div>

        <div class="section">
            <h2 class="section-title">Background: Semantic Analysis</h2>
            <div class="semantic-analysis-flex">
                <ul class="semantic-bullets">
                    <li>Formal semantic analysis allows to interpret embodied conversation messages and their <strong>logical consequences</strong></li>
                    <li><strong>Sentence-level analysis:</strong> "Put the two granny smiths inside a basket" entails that "there are only two granny smiths"</li>
                    <li><strong>Discourse-level analysis:</strong> correction on pick with message "No. This is golden delicious" entails that the picked object "is not granny smith"</li>
                </ul>
                <img src="assets/pointing.png" alt="Pointing Iliustation" class="semantic-image">
            </div>
        </div>


        <div class="section">
            <h2 class="section-title">Framework Overview</h2>
            <div class="semantic-analysis-flex">
                <ul class="semantic-bullets">
                    <li><strong>Agent's belief state</strong> contains domain theory build over the course of embodied conversation and examplars of observation-symbol pairs from experience.</li>
                    <li><strong>Dialogue strategy</strong> measures the value of asking certain questions to the teacher.</li>
                    <li>Query value is measured using expected information gain: \(\mathbb{I}[b,a] = \mathbb{H}[b] - \mathbb{E}_{\phi \sim \mathrm{Result}(a)}[\mathbb{H}[\mathrm{Update}(b,\phi)]]\) </li>
                    <li>It is included in the value function in weigthing query value and extected reward that includes the cost of wrong prediction: \(Q(b,a)=\theta_1\mathbb{I}[b,a]+\theta_2\mathbb{E}_{b}[R(a)]\)</li>
                </ul>
                <img src="assets/task.png" alt="Task Iliustation" class="semantic-image">
            </div>
        </div>

        <div class="section">
            <h2 class="section-title">Belief Update Examples</h2>
            <div class="image-container">
                <video width="45%" height="300" controls> <source src="assets/conv-granny-smith-4.mp4" type="video/mp4"> Your browser does not support the video tag.</video> 
                <video width="45%" height="300" controls> <source src="assets/corr-granny-smith-4.mp4" type="video/mp4"> Your browser does not support the video tag.</video> <br>
                <em>Figure 2: human-robot interaction using embodied conversation to ask a question to reduce the uncertainty <br> about the domain or to processes user's corrective feedback in case of a wrong actions</em>
            </div>

        </div>

        <div class="section">
            <h2 class="section-title">Experiments and Results</h2>
            <ul>    
                <li>We evaluate different agents in the <strong>simulated blocks domain</strong> and a <strong>real-world fruit domain</strong>, in which agents start unaware of the domain-level concepts and <strong>thought interaction learns to ground newly discovered concepts</strong></li> 
                <li>Engaging in embodied conversation and processing it using formal semantic analysis has <strong>compounding benefits</strong> for bootstrapping interactive task learning</li>
                <li>Semantics-aware agents can <strong>cope with false initial beliefs</strong> and revise them using the evidence acquired from extended interaction in the domain </li>
            </ul>
        </div>
    
        <div class="bibtex-section">
            <h2 class="section-title">Citation</h2>
            <button class="copy-button" onclick="copyBibtex()">Copy BibTeX</button>
            <div class="bibtex-code" id="bibtex" style="font-size: 12px;">@misc{secure2025, <br> title={SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning}, <br> author={Rimvydas Rubavicius and Peter David Fagan and Alex Lascarides and Subramanian Ramamoorthy}, <br> year={2025}, <br> eprint={2409.17755}, <br> archivePrefix={arXiv}, <br> primaryClass={cs.RO}, <br> url={https://arxiv.org/abs/2409.17755}, <br> }
</div>
        </div>
    </div>

    <script>
        // Function to copy BibTeX to clipboard
        function copyBibtex() {
            const bibtexText = document.getElementById('bibtex').textContent;
            navigator.clipboard.writeText(bibtexText).then(() => {
                const button = document.querySelector('.copy-button');
                button.textContent = 'Copied!';
                setTimeout(() => {
                    button.textContent = 'Copy BibTeX';
                }, 2000);
            });
        }

    </script>
</body>
</html>